# 环保文献智能数据库 - 详细部署说明

本文档将指导您一步步完成系统的本地部署。即使您没有编程经验，只要按照步骤操作，也可以成功运行这个演示系统。

---

## 目录

1. [系统要求](#1-系统要求)
2. [下载项目代码](#2-下载项目代码)
3. [安装准备软件](#3-安装准备软件)
4. [启动AI服务 (Ollama)](#4-启动ai服务-ollama)
5. [启动后端服务](#5-启动后端服务)
6. [启动前端服务](#6-启动前端服务)
7. [使用系统](#7-使用系统)
8. [常见问题解答](#8-常见问题解答)

---

## 1. 系统要求

在开始之前，请确保您的电脑满足以下要求：

- **操作系统**: Windows 10/11, macOS 10.15+, 或 Linux
- **内存**: 至少 16GB RAM (推荐 32GB，因为需要运行AI模型)
- **硬盘空间**: 至少 20GB 可用空间
- **网络**: 首次运行需要联网下载模型和依赖

---

## 2. 下载项目代码

### 方法一: 使用 Git 克隆 (推荐)

如果您的电脑已安装 Git，打开命令行/终端，运行:

```bash
git clone https://github.com/tyang4-ai/Random-testing-repo.git
```

克隆完成后，项目位于 `Random-testing-repo/ai-article-database/` 文件夹中。

### 方法二: 直接下载 ZIP 文件

如果没有安装 Git:

1. 打开浏览器，访问: https://github.com/tyang4-ai/Random-testing-repo
2. 点击绿色的 **"Code"** 按钮
3. 选择 **"Download ZIP"**
4. 下载完成后，解压 ZIP 文件到您想要的位置
5. 项目位于解压后的 `Random-testing-repo-main/ai-article-database/` 文件夹中

### 方法三: 安装 Git 后克隆

**Windows:**
1. 访问 https://git-scm.com/download/win
2. 下载并安装 Git
3. 安装完成后，打开"命令提示符"或"Git Bash"
4. 运行克隆命令

**macOS:**
```bash
# 使用 Homebrew 安装
brew install git

# 然后克隆项目
git clone https://github.com/tyang4-ai/Random-testing-repo.git
```

---

## 3. 安装准备软件

### 3.1 安装 Python

Python 是运行后端服务的编程语言。

**Windows 用户:**
1. 打开浏览器，访问 https://www.python.org/downloads/
2. 点击黄色的 "Download Python 3.12.x" 按钮
3. 运行下载的安装程序
4. **重要**: 勾选 "Add Python to PATH" 选项
5. 点击 "Install Now"

**macOS 用户:**
1. 打开"终端"应用 (在"应用程序" → "实用工具"中)
2. 输入以下命令并按回车:
   ```
   brew install python
   ```
   (如果没有安装 Homebrew，先访问 https://brew.sh 安装)

**验证安装:**
打开命令行/终端，输入:
```
python --version
```
应该显示 "Python 3.x.x"

---

### 3.2 安装 Node.js

Node.js 是运行前端服务的工具。

**所有系统:**
1. 访问 https://nodejs.org/
2. 下载 "LTS" 版本 (长期支持版)
3. 运行安装程序，一直点击"下一步"即可

**验证安装:**
打开命令行/终端，输入:
```
node --version
npm --version
```
都应该显示版本号

---

### 3.3 安装 Ollama (AI引擎)

Ollama 是运行 DeepSeek AI 模型的工具。

**Windows 用户:**
1. 访问 https://ollama.com/download
2. 点击 "Download for Windows"
3. 运行安装程序

**macOS 用户:**
1. 访问 https://ollama.com/download
2. 点击 "Download for macOS"
3. 将 Ollama 拖入"应用程序"文件夹

**Linux 用户:**
打开终端，运行:
```
curl -fsSL https://ollama.com/install.sh | sh
```

---

## 4. 启动AI服务 (Ollama)

### 4.1 下载AI模型

系统支持多种AI模型，推荐使用 **Qwen2 (通义千问)**，对中文支持最好。

打开命令行/终端，输入以下命令:

**推荐 - Qwen2 模型 (中文效果最好):**
```bash
ollama pull qwen2:7b
```

**备选模型 (如果上面的下载失败):**
```bash
# 较小的 Qwen 模型 (内存要求低)
ollama pull qwen2:1.5b

# 或者尝试其他模型
ollama pull llama3:8b
ollama pull gemma2:9b
```

> **提示**: 系统会自动检测已安装的模型并使用最合适的一个，所以只需要成功下载任意一个模型即可。

---

### 4.2 国内网络问题解决方案

如果下载模型时遇到网络错误 (如 "no such host" 或下载很慢)，请尝试以下方法:

#### 方法一: 使用代理 (如果有VPN)

在命令行中设置代理后再下载:

**Windows:**
```cmd
set HTTP_PROXY=http://127.0.0.1:7890
set HTTPS_PROXY=http://127.0.0.1:7890
ollama pull qwen2:7b
```

**macOS/Linux:**
```bash
export HTTP_PROXY=http://127.0.0.1:7890
export HTTPS_PROXY=http://127.0.0.1:7890
ollama pull qwen2:7b
```

> 将 `7890` 替换为您的代理端口号

#### 方法二: 手动复制模型文件

1. 找一台网络畅通的电脑下载模型
2. 复制模型文件夹到目标电脑:
   - **Windows**: `C:\Users\<用户名>\.ollama\models\`
   - **macOS/Linux**: `~/.ollama/models/`

#### 方法三: 尝试更小的模型

较小的模型下载更快:
```bash
ollama pull qwen2:1.5b    # 约1GB
ollama pull phi:latest    # 约1.6GB
```

---

### 4.3 启动 Ollama 服务

**Windows:**
- 通常安装后会自动启动
- 也可以在开始菜单搜索 "Ollama" 并运行

**macOS:**
- 在"应用程序"中双击 Ollama 图标
- 或在终端运行: `ollama serve`

**Linux:**
```bash
ollama serve
```

**验证服务是否运行:**
打开浏览器，访问 http://localhost:11434
如果显示 "Ollama is running"，说明启动成功

---

## 5. 启动后端服务

### 5.1 打开项目目录

**Windows:**
1. 按 Win+R，输入 `cmd`，按回车打开命令提示符
2. 输入以下命令进入项目目录:
   ```
   cd 您的项目路径\ai-article-database\backend
   ```

**macOS/Linux:**
1. 打开终端
2. 输入:
   ```
   cd 您的项目路径/ai-article-database/backend
   ```

### 5.2 创建虚拟环境 (推荐)

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\Scripts\activate

# macOS/Linux:
source venv/bin/activate
```

激活后，命令行前面会出现 `(venv)` 标志

### 5.3 安装后端依赖

```bash
pip install -r requirements.txt
```

这会安装所有需要的 Python 库。首次安装可能需要几分钟。

### 5.4 导入示例数据

首次运行需要导入示例文章数据:

```bash
python load_sample_data.py
```

**注意**: 首次运行会自动下载 BGE 中文嵌入模型 (约1.3GB)，请耐心等待。

导入成功后，您会看到:
```
✅ 成功导入 10 篇文章!
```

### 5.5 启动后端服务

```bash
python main.py
```

启动成功后，您会看到:
```
==================================================
环保文献智能数据库启动成功!
后端API地址: http://localhost:8000
API文档地址: http://localhost:8000/docs
==================================================
```

**保持这个终端窗口开着，不要关闭!**

---

## 6. 启动前端服务

### 6.1 打开新的命令行窗口

**重要**: 不要关闭后端的终端窗口，打开一个新的终端窗口

### 6.2 进入前端目录

```bash
cd 您的项目路径/ai-article-database/frontend
```

### 6.3 安装前端依赖

```bash
npm install
```

首次安装可能需要几分钟。

### 6.4 启动前端服务

```bash
npm run dev
```

启动成功后，您会看到:
```
  VITE v5.x.x  ready in xxx ms

  ➜  Local:   http://localhost:5173/
```

---

## 7. 使用系统

### 7.1 打开系统界面

打开浏览器，访问: **http://localhost:5173**

您将看到环保文献智能数据库的主界面。

### 7.2 用户界面 - 文献检索

1. 在搜索框输入关键词，如: `长三角 水污染`
2. 点击"搜索"按钮
3. 系统会显示相关的文章列表，按相关度排序
4. 勾选您感兴趣的文章
5. 点击"生成报告"按钮
6. 等待AI生成研究报告 (约1-2分钟)
7. 报告生成后可以下载为Markdown文件

### 7.3 管理员界面 - 文章管理

1. 点击顶部导航栏的"文章管理"
2. 查看现有文章列表
3. 点击"添加文章"可以添加新文章
4. 点击文章行的"编辑"可以修改文章
5. 点击"删除"可以删除文章

---

## 8. 常见问题解答

### Q1: 搜索没有结果？

**可能原因:**
- 数据库中没有导入文章
- 关键词与文章内容相关度太低

**解决方法:**
- 确认已运行 `python load_sample_data.py` 导入示例数据
- 尝试不同的关键词，如: "苏州河"、"碳中和"、"生态保护"

---

### Q2: 生成报告失败，提示"AI服务未启动"？

**解决方法:**
1. 确认 Ollama 正在运行
2. 打开新终端，运行: `ollama serve`
3. 确认已下载模型: `ollama pull qwen2:7b`

---

### Q2.5: 提示"没有可用的AI模型"？

**解决方法:**
需要下载至少一个AI模型。运行以下命令之一:
```bash
ollama pull qwen2:7b      # 推荐
ollama pull qwen2:1.5b    # 较小，内存要求低
ollama pull llama3:8b     # 备选
```

如果遇到网络问题，请参考 [4.2 国内网络问题解决方案](#42-国内网络问题解决方案)

---

### Q3: 后端启动失败，提示"找不到模块"？

**解决方法:**
1. 确认已激活虚拟环境 (命令行前有 `(venv)`)
2. 重新安装依赖: `pip install -r requirements.txt`

---

### Q4: 前端启动失败？

**解决方法:**
1. 确认已安装 Node.js
2. 删除 `node_modules` 文件夹
3. 重新运行: `npm install`
4. 再次运行: `npm run dev`

---

### Q5: 首次运行很慢？

这是正常现象。首次运行需要:
1. 下载 BGE 嵌入模型 (~1.3GB)
2. 加载模型到内存

之后的运行会快很多。

---

### Q5.5: 下载模型时提示 "no such host" 或网络错误？

这是国内网络访问国外服务器的问题。请参考 [4.2 国内网络问题解决方案](#42-国内网络问题解决方案)。

简单总结:
1. 如果有VPN，设置代理后再下载
2. 或者找网络畅通的电脑下载后复制模型文件
3. 或者尝试下载更小的模型: `ollama pull qwen2:1.5b`

---

### Q6: 如何停止服务？

在对应的终端窗口按 `Ctrl+C` 即可停止服务。

---

### Q7: 如何添加自己的文章？

方法一: 通过管理界面
1. 访问 http://localhost:5173/admin
2. 点击"添加文章"
3. 填写文章信息并保存

方法二: 修改示例数据文件
1. 编辑 `data/sample_articles.json`
2. 按照现有格式添加新文章
3. 重新运行 `python load_sample_data.py`

---

## 快速启动命令汇总

每次使用系统时，需要启动三个服务:

**终端1 - Ollama (AI服务):**
```bash
ollama serve
```

**终端2 - 后端服务:**
```bash
cd ai-article-database/backend
source venv/bin/activate  # Windows: venv\Scripts\activate
python main.py
```

**终端3 - 前端服务:**
```bash
cd ai-article-database/frontend
npm run dev
```

然后打开浏览器访问: http://localhost:5173

---

## 联系支持

如有问题，请联系技术支持。

---

*环保文献智能数据库 v1.1.0*

*支持模型: Qwen2, DeepSeek, Llama3, Gemma2, Mistral 等*
